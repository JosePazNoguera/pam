{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:48:47.722892Z",
     "start_time": "2020-11-10T19:48:46.948691Z"
    }
   },
   "source": [
    "# Converting the National Travel Survey into a Simple MATSim Format Population\n",
    "\n",
    "This notebook demonstrates an example workflow for converting tabular diary data (household attributes, person attributes and trip data) into MATSim formatted xml population data for London households.\n",
    "\n",
    "This includes:\n",
    "- pre-processing of tabular inputs\n",
    "- loading data into pam\n",
    "- household sampling\n",
    "- facility sampling\n",
    "- preliminary investigation\n",
    "- writing to xml\n",
    "\n",
    "This example is highly simplified. Of particular note: the diary data used is spatially very aggregate (trip locations are aggregated to inner/outer London). This creates significant variance in the sampled trip lengths. Generally we would expect more precise spatial data to be used. Alternately the complexity of the facility sampling step can be improved to better account for known trip features such as mode and duration.\n",
    "\n",
    "The diary data used is available from the UK Data Service (https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=5340) and is described here:http://doc.ukdataservice.ac.uk/doc/5340/mrdoc/pdf/5340_nts_user_guidance_1995-2016.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:19.779483Z",
     "start_time": "2020-11-11T11:50:19.172087Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:19.807018Z",
     "start_time": "2020-11-11T11:50:19.783419Z"
    }
   },
   "outputs": [],
   "source": [
    "out_dir = '/outputs'  # outputs are writen here\n",
    "\n",
    "# required inputs from the National Travel Survey\n",
    "households_csv = '~/Data/UKDA-5340-tab/tab/householdeul2017.tab'\n",
    "individuals_csv = '~/Data/UKDA-5340-tab/tab/individualeul2017.tab'\n",
    "trips_csv ='~/Data/UKDA-5340-tab/tab/tripeul2017.tab'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T17:13:00.123065Z",
     "start_time": "2020-08-03T17:13:00.093023Z"
    }
   },
   "source": [
    "## Load households data\n",
    "\n",
    "1. Load household data into pandas DataFrame.\n",
    "2. Create some mappings of participation and weighting by household for use later. These are described in http://doc.ukdataservice.ac.uk/doc/5340/mrdoc/pdf/5340_nts_user_guidance_1995-2016.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:21.160817Z",
     "start_time": "2020-11-11T11:50:19.812095Z"
    }
   },
   "outputs": [],
   "source": [
    "hh_in = pd.read_csv(\n",
    "    households_csv,\n",
    "    sep='\\t',\n",
    "    usecols=['HouseholdID', 'SurveyYear', 'PSUID', 'W2', 'OutCom_B02ID',\n",
    "       'HHIncome2002_B02ID', 'AddressType_B01ID', 'Ten1_B02ID',\n",
    "       'Landlord_B01ID', 'ResLength_B01ID', 'HHoldCountry_B01ID',\n",
    "       'HHoldGOR_B02ID', 'HHoldNumAdults', 'HHoldNumChildren',\n",
    "       'HHoldNumPeople', 'HHoldStruct_B02ID', 'NumLicHolders',\n",
    "       'HHoldEmploy_B01ID', 'NumVehicles', 'NumBike', 'NumCar', 'NumMCycle',\n",
    "       'NumVanLorry', 'NumCarVan', 'WalkBus_B01ID', 'Getbus_B01ID',\n",
    "       'WalkRail_B01ID', 'WalkRailAlt_B01ID',\n",
    "       'HRPWorkStat_B02ID', 'HRPSEGWorkStat_B01ID', 'HHoldOAClass2011_B03ID',\n",
    "       'Settlement2011EW_B03ID', 'Settlement2011EW_B04ID'],\n",
    ")\n",
    "\n",
    "hh_in.HHIncome2002_B02ID = pd.to_numeric(hh_in.HHIncome2002_B02ID, errors='coerce')\n",
    "hh_in.NumLicHolders = pd.to_numeric(hh_in.NumLicHolders, errors='coerce')\n",
    "hh_in.NumVehicles = pd.to_numeric(hh_in.NumVehicles, errors='coerce')\n",
    "hh_in.NumCar = pd.to_numeric(hh_in.NumCar, errors='coerce')\n",
    "hh_in.NumMCycle = pd.to_numeric(hh_in.NumMCycle, errors='coerce')\n",
    "hh_in.NumVanLorry = pd.to_numeric(hh_in.NumVanLorry, errors='coerce')\n",
    "hh_in.NumCarVan = pd.to_numeric(hh_in.NumCarVan, errors='coerce')\n",
    "hh_in.Settlement2011EW_B04ID = pd.to_numeric(hh_in.Settlement2011EW_B04ID, errors='coerce')\n",
    "\n",
    "hh_in.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:21.261211Z",
     "start_time": "2020-11-11T11:50:21.164403Z"
    }
   },
   "outputs": [],
   "source": [
    "participation_mapping = dict(zip(hh_in.HouseholdID, hh_in.OutCom_B02ID))\n",
    "weight_mapping = dict(zip(hh_in.HouseholdID, hh_in.W2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T10:49:47.862682Z",
     "start_time": "2020-08-04T10:49:47.799479Z"
    }
   },
   "source": [
    "## Load person data\n",
    "\n",
    "Load person attributes data into pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:24.889954Z",
     "start_time": "2020-11-11T11:50:21.263760Z"
    }
   },
   "outputs": [],
   "source": [
    "persons_in = pd.read_csv(\n",
    "    individuals_csv,\n",
    "    sep='\\t',\n",
    "    usecols=['SurveyYear', 'IndividualID', 'HouseholdID', 'PSUID', 'VehicleID',\n",
    "       'PersNo', 'Age_B01ID', 'OfPenAge_B01ID', 'Sex_B01ID', 'EdAttn1_B01ID',\n",
    "       'EdAttn2_B01ID', 'EdAttn3_B01ID', 'DrivLic_B02ID', 'CarAccess_B01ID',\n",
    "       'DrivDisable_B01ID', 'WkPlace_B01ID', 'ES2000_B01ID', 'NSSec_B03ID',\n",
    "       'SC_B01ID', 'Stat_B01ID', 'SVise_B01ID', 'EcoStat_B02ID',\n",
    "       'PossHom_B01ID']\n",
    ")\n",
    "persons_in.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load trip data\n",
    "\n",
    "1. Load trip data into pandas DataFrame format.\n",
    "2. Apply some preliminary formatting\n",
    "3. Replace headers so that we can use pam read method:\n",
    "\n",
    "\n",
    "- pid - person ID\n",
    "- hid - household ID\n",
    "- seq - trip sequence number\n",
    "- hzone - household zone\n",
    "- ozone - trip origin zone\n",
    "- dzone - trip destination zone\n",
    "- purp - trip purpose\n",
    "- mode - trip mode\n",
    "- tst - trip start time (minutes)\n",
    "- tet - trip end time (minutes)\n",
    "- freq - weighting for representative population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:39.717878Z",
     "start_time": "2020-11-11T11:50:24.893543Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries_in = pd.read_csv(\n",
    "    trips_csv,\n",
    "    sep='\\t',\n",
    "    usecols=['TripID', 'SurveyYear', 'DayID', 'IndividualID', 'HouseholdID', 'PSUID',\n",
    "       'PersNo', 'TravDay', 'JourSeq', 'ShortWalkTrip_B01ID', 'NumStages',\n",
    "       'MainMode_B04ID', 'TripPurpFrom_B01ID',\n",
    "        'TripPurpTo_B01ID', 'TripPurpose_B04ID',\n",
    "       'TripStart', 'TripEnd', 'TripOrigUA2009_B01ID', 'TripDestUA2009_B01ID'],\n",
    "#     dtype={\"W5\": np.float64,}\n",
    ")\n",
    "\n",
    "travel_diaries_in.TripStart = pd.to_numeric(travel_diaries_in.TripStart, errors='coerce')\n",
    "travel_diaries_in.TripEnd = pd.to_numeric(travel_diaries_in.TripEnd, errors='coerce')\n",
    "\n",
    "travel_diaries_in.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:40.105779Z",
     "start_time": "2020-11-11T11:50:39.723850Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries_in['participation'] = travel_diaries_in.HouseholdID.map(participation_mapping)\n",
    "travel_diaries_in['hh_weight'] = travel_diaries_in.HouseholdID.map(weight_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.188471Z",
     "start_time": "2020-11-11T11:50:40.108909Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries = travel_diaries_in.loc[travel_diaries_in.participation.isin([1,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.232228Z",
     "start_time": "2020-11-11T11:50:41.190789Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.330726Z",
     "start_time": "2020-11-11T11:50:41.238098Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries.rename(\n",
    "    columns={  # rename data\n",
    "        'JourSeq': 'seq',\n",
    "        'TripOrigUA2009_B01ID': 'ozone',\n",
    "        'TripDestUA2009_B01ID': 'dzone',\n",
    "        'TripPurpFrom_B01ID': 'oact',\n",
    "        'TripPurpTo_B01ID': 'dact',\n",
    "        'MainMode_B04ID': 'mode',\n",
    "        'TripStart': 'tst',\n",
    "        'TripEnd': 'tet',\n",
    "    },\n",
    "                inplace=True)\n",
    "\n",
    "travel_diaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.371136Z",
     "start_time": "2020-11-11T11:50:41.339066Z"
    }
   },
   "outputs": [],
   "source": [
    "travel_diaries.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.398184Z",
     "start_time": "2020-11-11T11:50:41.374372Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_uniques(df):\n",
    "    for c in df.columns:\n",
    "        print(c)\n",
    "        n = df[c].nunique()\n",
    "        if n < 1000:\n",
    "            print(df[c].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.543524Z",
     "start_time": "2020-11-11T11:50:41.402704Z"
    }
   },
   "outputs": [],
   "source": [
    "check_uniques(travel_diaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatially Filter Plans\n",
    "\n",
    "For this example we are only going to consider individuals who travel entirely within London. The NTS data we are using has extremely aggregate spatial data for trips:\n",
    "\n",
    "```\n",
    "london_areas = {\n",
    "    360.0: 'London Central',\n",
    "    800.0: 'Inner London - excluding Central London',\n",
    "    370.0: 'Outer London',\n",
    "}\n",
    "```\n",
    "\n",
    "We further simoplify this to Inner and Outer London only. This Inner/Outer London geometry is based on https://data.gov.uk/dataset/f9e5c66f-954a-4633-af9a-081318726548/inner-and-outer-london-boundaries-london-plan-consolidated-with-alterations-since-2004. Note that this data is in `EPSG:27700` which we will use for facility sampling so that our MATSim population's plans use metres as distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.569070Z",
     "start_time": "2020-11-11T11:50:41.545823Z"
    }
   },
   "outputs": [],
   "source": [
    "areas_shp = '~/Data/UKDA-5340-tab/lp-falp-2006-inner-outer-london-shp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.837495Z",
     "start_time": "2020-11-11T11:50:41.572085Z"
    }
   },
   "outputs": [],
   "source": [
    "areas = gp.read_file(areas_shp).set_index(\"Designated\")\n",
    "areas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.845409Z",
     "start_time": "2020-11-11T11:50:18.812Z"
    }
   },
   "outputs": [],
   "source": [
    "areas.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.848714Z",
     "start_time": "2020-11-11T11:50:18.815Z"
    }
   },
   "outputs": [],
   "source": [
    "new_mapping = {\n",
    "    360.0: 'Inner',\n",
    "    800.0: 'Inner',\n",
    "    370.0: 'Outer',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.859730Z",
     "start_time": "2020-11-11T11:50:18.818Z"
    }
   },
   "outputs": [],
   "source": [
    "def area_filter(df):\n",
    "    return not set(list(df.ozone) + list(df.dzone)) - {360, 800, 370}\n",
    "        \n",
    "\n",
    "london_travel_diaries = travel_diaries.groupby('IndividualID').filter(area_filter)\n",
    "london_travel_diaries.ozone = london_travel_diaries.ozone.map(new_mapping)\n",
    "london_travel_diaries.dzone = london_travel_diaries.dzone.map(new_mapping)\n",
    "\n",
    "print(len(travel_diaries), \"-->\", len(london_travel_diaries))\n",
    "\n",
    "london_travel_diaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean out incomplete plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.872876Z",
     "start_time": "2020-11-11T11:50:18.820Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_broken_plans(plan):\n",
    "    if plan.isnull().values.any():\n",
    "        return None\n",
    "    for col in ['ozone', 'dzone']:\n",
    "        if -8 in list(plan[col]):\n",
    "            return None\n",
    "    return plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.874642Z",
     "start_time": "2020-11-11T11:50:18.824Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_travel_diaries = london_travel_diaries.groupby(\n",
    "    ['IndividualID', 'TravDay']\n",
    ").apply(\n",
    "    remove_broken_plans\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.877284Z",
     "start_time": "2020-11-11T11:50:18.827Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_travel_diaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.881055Z",
     "start_time": "2020-11-11T11:50:18.830Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(travel_diaries))\n",
    "print(len(clean_travel_diaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Mappings and apply to common fields\n",
    "\n",
    "We simplify key trip variables such as mode and activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.892724Z",
     "start_time": "2020-11-11T11:50:18.832Z"
    }
   },
   "outputs": [],
   "source": [
    "def string_to_dict(string):\n",
    "    \"\"\"used to build dicts from NTS rtf format dictionaries (cut and paste from the NTS documentation)\"\"\"\n",
    "    mapping = {}\n",
    "    for line in string.split(\"\\n\"):\n",
    "        _, v, l = line.split(\"\\t\")\n",
    "        v = v.split(\" = \")[1]\n",
    "        l = l.split(\" = \")[1]\n",
    "        mapping[float(v)] = str(l)\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.893948Z",
     "start_time": "2020-11-11T11:50:18.835Z"
    }
   },
   "outputs": [],
   "source": [
    "mode_mapping = {\n",
    "    1: 'walk',\n",
    "     2: 'bike',\n",
    "     3: 'car',  #'Car/van driver'\n",
    "     4: 'car',  #'Car/van driver'\n",
    "     5: 'car',  #'Motorcycle',\n",
    "     6: 'car',  #'Other private transport',\n",
    "     7: 'pt', #Bus in London',\n",
    "     8: 'pt', #'Other local bus',\n",
    "     9: 'pt', #'Non-local bus',\n",
    "     10: 'pt', #'London Underground',\n",
    "     11: 'pt', #'Surface Rail',\n",
    "     12: 'car',  #'Taxi/minicab',\n",
    "     13: 'pt', #'Other public transport',\n",
    "     -10: 'DEAD',\n",
    "     -8: 'NA'\n",
    "}\n",
    "\n",
    "purp_mapping = {\n",
    "    1: 'work',\n",
    "     2: 'work',  #'In course of work',\n",
    "     3: 'education',\n",
    "     4: 'shop',  #'Food shopping',\n",
    "     5: 'shop',  #'Non food shopping',\n",
    "     6: 'medical', #'Personal business medical',\n",
    "     7: 'other',  #'Personal business eat/drink',\n",
    "     8: 'other',  #'Personal business other',\n",
    "     9: 'other',  #'Eat/drink with friends',\n",
    "     10: 'visit',  #'Visit friends',\n",
    "     11: 'other',  #'Other social',\n",
    "     12: 'other',  #'Entertain/ public activity',\n",
    "     13: 'other',  #'Sport: participate',\n",
    "     14: 'home',  #'Holiday: base',\n",
    "     15: 'other',  #'Day trip/just walk',\n",
    "     16: 'other',  #'Other non-escort',\n",
    "     17: 'escort',  #'Escort home',\n",
    "     18: 'escort',  #'Escort work',\n",
    "     19: 'escort',  #'Escort in course of work',\n",
    "     20: 'escort',  #'Escort education',\n",
    "     21: 'escort',  #'Escort shopping/personal business',\n",
    "     22: 'escort',  #'Other escort',\n",
    "     23: 'home',  #'Home',\n",
    "     -10: 'DEAD',\n",
    "     -8: 'NA'\n",
    "}\n",
    "\n",
    "clean_travel_diaries['mode'] = clean_travel_diaries['mode'].map(mode_mapping)\n",
    "clean_travel_diaries['oact'] = clean_travel_diaries['oact'].map(purp_mapping)\n",
    "clean_travel_diaries['dact'] = clean_travel_diaries['dact'].map(purp_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reweight and Split Days\n",
    "\n",
    "In order to get the most from our small sample we treat individual diary days as new persons. In order to maintain the original household weighting we reduce this accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.905596Z",
     "start_time": "2020-11-11T11:50:18.838Z"
    }
   },
   "outputs": [],
   "source": [
    "# reweight and split ids for unique days\n",
    "\n",
    "def reweight(group):\n",
    "    \"\"\"\n",
    "    Reweight based on multiple diary days, ie if an agent has two diary days, we will treat these as\n",
    "    two unique agents, so we half the original weighting\n",
    "    \"\"\"\n",
    "    group['freq'] = group.hh_weight / group.DayID.nunique()\n",
    "    return group\n",
    "\n",
    "trips = clean_travel_diaries.groupby('IndividualID').apply(reweight)\n",
    "trips['pid'] = [f\"{p}_{d}\" for p, d in zip(trips.IndividualID, trips.TravDay)]\n",
    "trips['hid'] = [f\"{h}_{d}\" for h, d in zip(trips.HouseholdID, trips.TravDay)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.912536Z",
     "start_time": "2020-11-11T11:50:18.841Z"
    }
   },
   "outputs": [],
   "source": [
    "trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.914516Z",
     "start_time": "2020-11-11T11:50:18.845Z"
    }
   },
   "outputs": [],
   "source": [
    "def expand_days(\n",
    "    trips,\n",
    "    target,\n",
    "    trips_on='Diary_number',\n",
    "    target_on='Diary_number',\n",
    "    new_id='pid',\n",
    "    trim=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Expand target df based on mapping between trips target_on and new_id.\n",
    "    Set index to new_id.\n",
    "    \"\"\"\n",
    "    print(\"Building mapping.\")\n",
    "    mapping = {}\n",
    "    for i, person in trips.groupby(target_on):\n",
    "        mapping[i] = list(set(person[new_id]))\n",
    "    n = len(mapping)\n",
    "    \n",
    "    if trim:\n",
    "        print(\"Trimming target.\")\n",
    "        selection = set(trips[trips_on])\n",
    "        target = target.loc[target[target_on].isin(selection)]\n",
    "    \n",
    "    expanded = pd.DataFrame()\n",
    "    for p, (i, ids) in enumerate(mapping.items()):\n",
    "        if not p % 10:\n",
    "            print(f\"Building expanded data {p}/{n}\", end='\\r', flush=True)\n",
    "        for idx in ids:\n",
    "            split = target.loc[target[target_on] == i]\n",
    "            split[new_id] = idx\n",
    "            expanded = expanded.append(split)\n",
    "    expanded.set_index(new_id, inplace=True)\n",
    "    print(f\"Done\")\n",
    "    return expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.927413Z",
     "start_time": "2020-11-11T11:50:18.848Z"
    }
   },
   "outputs": [],
   "source": [
    "hhs = expand_days(\n",
    "    trips,\n",
    "    hh_in,\n",
    "    trips_on='HouseholdID',\n",
    "    target_on='HouseholdID',\n",
    "    new_id='hid'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.937528Z",
     "start_time": "2020-11-11T11:50:18.851Z"
    }
   },
   "outputs": [],
   "source": [
    "hhs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.941545Z",
     "start_time": "2020-11-11T11:50:18.853Z"
    }
   },
   "outputs": [],
   "source": [
    "people = expand_days(\n",
    "    trips,\n",
    "    persons_in,\n",
    "    trips_on='IndividualID',\n",
    "    target_on='IndividualID',\n",
    "    new_id='pid'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.945384Z",
     "start_time": "2020-11-11T11:50:18.857Z"
    }
   },
   "outputs": [],
   "source": [
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T16:26:28.285970Z",
     "start_time": "2020-11-05T16:26:28.140318Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Mapping - NOT WORKING\n",
    "\n",
    "We are currently using a London only example in part because at the time of writing no geometries have been found for the trip origin and destination encodings. These are included below for reference. The encodings are similar but not the same as those here: https://data.gov.uk/dataset/4e1d5b2c-bb91-42ad-b420-f7fcab638389/counties-and-unitary-authorities-december-2017-full-extent-boundaries-in-uk-wgs84."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.956640Z",
     "start_time": "2020-11-11T11:50:18.862Z"
    }
   },
   "outputs": [],
   "source": [
    "area_mapping = {\n",
    "    520.0: 'Warwickshire',\n",
    "     530.0: 'West Midlands',\n",
    "     540.0: 'West Sussex',\n",
    "     191.0: 'Derby',\n",
    "     550.0: 'West Yorkshire',\n",
    "     560.0: 'Wiltshire',\n",
    "     561.0: 'Swindon',\n",
    "     601.0: 'Isle of Anglesey',\n",
    "     602.0: 'Gwynedd',\n",
    "     603.0: 'Conwy',\n",
    "     604.0: 'Denbighshire',\n",
    "     605.0: 'Flintshire',\n",
    "     606.0: 'Wrexham',\n",
    "     607.0: 'Powys',\n",
    "     608.0: 'Ceredigion',\n",
    "     609.0: 'Pembrokeshire',\n",
    "     610.0: 'Carmarthenshire',\n",
    "     611.0: 'Swansea',\n",
    "     612.0: 'Neath and Port Talbot',\n",
    "     101.0: 'Bath and N.E. Somerset',\n",
    "     614.0: 'Vale of Glamorgan',\n",
    "     103.0: 'North Somerset',\n",
    "     616.0: 'Rhondda, Cynon, Taff',\n",
    "     617.0: 'Merthyr Tydfil',\n",
    "     618.0: 'Caerphilly',\n",
    "     615.0: 'Cardiff',\n",
    "     620.0: 'Torfaen',\n",
    "     621.0: 'Monmouthshire',\n",
    "     622.0: 'Newport',\n",
    "     111.0: 'Luton',\n",
    "     112.0: 'Bedford',\n",
    "     104.0: 'South Gloucestershire',\n",
    "     190.0: 'Derbyshire',\n",
    "     121.0: 'Bracknell Forest',\n",
    "     122.0: 'Newbury',\n",
    "     123.0: 'Reading',\n",
    "     124.0: 'Slough',\n",
    "     125.0: 'Windsor & Maidenhead',\n",
    "     126.0: 'Wokingham',\n",
    "     130.0: 'Buckinghamshire',\n",
    "     131.0: 'Milton Keynes',\n",
    "     140.0: 'Cambridgeshire',\n",
    "     141.0: 'Peterborough',\n",
    "     151.0: 'Halton',\n",
    "     152.0: 'Warrington',\n",
    "     153.0: 'Cheshire East',\n",
    "     154.0: 'Cheshire West and Chester',\n",
    "     161.0: 'Hartlepool',\n",
    "     162.0: 'Middlesbrough',\n",
    "     163.0: 'Redcar & Cleveland',\n",
    "     164.0: 'Stockton-on-Tees',\n",
    "     113.0: 'Bedfordshire, Central',\n",
    "     170.0: 'Cornwall & Isles of Scilly',\n",
    "     200.0: 'Devon',\n",
    "     180.0: 'Cumbria',\n",
    "     201.0: 'Plymouth',\n",
    "     701.0: 'Aberdeen City',\n",
    "     702.0: 'Aberdeenshire',\n",
    "     703.0: 'Angus',\n",
    "     704.0: 'Argyll and Bute',\n",
    "     705.0: 'Scottish Borders',\n",
    "     706.0: 'Clackmannanshire',\n",
    "     707.0: 'West Dunbartonshire',\n",
    "     708.0: 'Dumfries and Galloway',\n",
    "     709.0: 'Dundee, City of',\n",
    "     710.0: 'East Ayrshire',\n",
    "     711.0: 'East Dunbartonshire',\n",
    "     712.0: 'East Lothian',\n",
    "     713.0: 'East Renfrewshire',\n",
    "     714.0: 'Edinburgh, City of',\n",
    "     715.0: 'Falkirk',\n",
    "     716.0: 'Fife',\n",
    "     717.0: 'Glasgow, City of',\n",
    "     718.0: 'Highland',\n",
    "     719.0: 'Inverclyde',\n",
    "     720.0: 'Midlothian',\n",
    "     721.0: 'Moray',\n",
    "     210.0: 'Dorset',\n",
    "     211.0: 'Bournemouth',\n",
    "     212.0: 'Poole',\n",
    "     725.0: 'Perth and Kinross',\n",
    "     726.0: 'Renfrewshire',\n",
    "     727.0: 'Shetland Islands',\n",
    "     728.0: 'South Ayrshire',\n",
    "     729.0: 'South Lanarkshire',\n",
    "     730.0: 'Stirling',\n",
    "     731.0: 'West Lothian',\n",
    "     732.0: 'Western Isles',\n",
    "     221.0: 'Darlington',\n",
    "     230.0: 'East Sussex',\n",
    "     231.0: 'Brighton and Hove',\n",
    "     722.0: 'North Ayrshire',\n",
    "     240.0: 'Essex - area outside M25',\n",
    "     241.0: 'Southend on Sea',\n",
    "     242.0: 'Thurrock',\n",
    "     723.0: 'North Lanarkshire',\n",
    "     724.0: 'Orkney Islands',\n",
    "     250.0: 'Gloucestershire',\n",
    "     202.0: 'Torbay',\n",
    "     260.0: 'Greater Manchester',\n",
    "     270.0: 'Hampshire',\n",
    "     271.0: 'Portsmouth',\n",
    "     272.0: 'Southampton',\n",
    "     280.0: 'Worcestershire',\n",
    "     281.0: 'Herefordshire',\n",
    "     800.0: 'Inner London - excluding Central London',\n",
    "     290.0: 'Hertfordshire - area outside M25',\n",
    "     220.0: 'Durham',\n",
    "     810.0: 'Essex - area within M25',\n",
    "     301.0: 'East Riding of Yorkshire',\n",
    "     302.0: 'Kingston upon Hull, City of',\n",
    "     303.0: 'North East Lincolnshire',\n",
    "     304.0: 'North Lincolnshire',\n",
    "     820.0: 'Hertfordshire - area within M25',\n",
    "     310.0: 'Isle of Wight',\n",
    "     830.0: 'Kent - area within M25',\n",
    "     320.0: 'Kent - area outside M25',\n",
    "     321.0: 'Medway Towns',\n",
    "     840.0: 'Surrey - area within M25',\n",
    "     330.0: 'Lancashire',\n",
    "     331.0: 'Blackburn with Darwen',\n",
    "     332.0: 'Blackpool',\n",
    "     340.0: 'Leicestershire',\n",
    "     341.0: 'Leicester',\n",
    "     342.0: 'Rutland',\n",
    "     350.0: 'Lincolnshire',\n",
    "     360.0: 'London Central',\n",
    "     370.0: 'Outer London',\n",
    "     380.0: 'Merseyside',\n",
    "     390.0: 'Norfolk',\n",
    "     400.0: 'Northamptonshire',\n",
    "     410.0: 'Northumberland',\n",
    "     420.0: 'North Yorkshire',\n",
    "     421.0: 'York',\n",
    "     430.0: 'Nottinghamshire',\n",
    "     431.0: 'Nottingham',\n",
    "     440.0: 'Oxfordshire',\n",
    "     450.0: 'Shropshire',\n",
    "     451.0: 'The Wrekin',\n",
    "     460.0: 'Somerset',\n",
    "     470.0: 'South Yorkshire',\n",
    "     480.0: 'Staffordshire',\n",
    "     481.0: 'Stoke-on-Trent',\n",
    "     490.0: 'Suffolk',\n",
    "     613.0: 'Bridgend',\n",
    "     619.0: 'Blaenau Gwent',\n",
    "     500.0: 'Surrey - area outside M25',\n",
    "     -10.0: 'DEAD',\n",
    "     -9.0: 'DNA',\n",
    "     -8.0: 'NA',\n",
    "     102.0: 'Bristol, City of',\n",
    "     510.0: 'Tyne and Wear'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load into PAM\n",
    "\n",
    "We load the pandas formatted data into Pam using the `pam.read.load_travel_diary_from_to` read method. We do some very preliminary validation of plans and assurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.959989Z",
     "start_time": "2020-11-11T11:50:18.865Z"
    }
   },
   "outputs": [],
   "source": [
    "from pam import write\n",
    "from pam import read\n",
    "from pam.plot.stats import plot_activity_times, plot_leg_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.963890Z",
     "start_time": "2020-11-11T11:50:18.867Z"
    }
   },
   "outputs": [],
   "source": [
    "trips.tst = trips.tst.astype(int)\n",
    "trips.tet = trips.tet.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.977259Z",
     "start_time": "2020-11-11T11:50:18.870Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "population = read.load_travel_diary_from_to(\n",
    "    trip_diary=trips,\n",
    "    person_attributes=people,\n",
    "    hh_attributes=hhs,\n",
    "    sample_perc=.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.987073Z",
     "start_time": "2020-11-11T11:50:18.872Z"
    }
   },
   "outputs": [],
   "source": [
    "population.fix_plans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.991657Z",
     "start_time": "2020-11-11T11:50:18.875Z"
    }
   },
   "outputs": [],
   "source": [
    "# this should be replaced with a more direct method\n",
    "for hh in population.households.values():\n",
    "    for p in hh.people.values():\n",
    "        p.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:41.998013Z",
     "start_time": "2020-11-11T11:50:18.877Z"
    }
   },
   "outputs": [],
   "source": [
    "population.size  # this also accounts for the weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.005374Z",
     "start_time": "2020-11-11T11:50:18.879Z"
    }
   },
   "outputs": [],
   "source": [
    "population.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.008270Z",
     "start_time": "2020-11-11T11:50:18.881Z"
    }
   },
   "outputs": [],
   "source": [
    "population.activity_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.022122Z",
     "start_time": "2020-11-11T11:50:18.882Z"
    }
   },
   "outputs": [],
   "source": [
    "population.mode_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.027053Z",
     "start_time": "2020-11-11T11:50:18.885Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_activity_times(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.028469Z",
     "start_time": "2020-11-11T11:50:18.887Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_leg_times(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.039323Z",
     "start_time": "2020-11-11T11:50:18.889Z"
    }
   },
   "outputs": [],
   "source": [
    "# night shift @ 2016008863_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.041452Z",
     "start_time": "2020-11-11T11:50:18.890Z"
    }
   },
   "outputs": [],
   "source": [
    "hh = population.random_household()\n",
    "hh.print()\n",
    "hh.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the Population\n",
    "\n",
    "We sample a very small population based on the given NTS household weightings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.053248Z",
     "start_time": "2020-11-11T11:50:18.892Z"
    }
   },
   "outputs": [],
   "source": [
    "from pam.core import Population\n",
    "from pam.samplers.basic import freq_sample\n",
    "from copy import deepcopy\n",
    "\n",
    "population_sample = Population()\n",
    "    \n",
    "for hid, household in population.households.items():\n",
    "    av_hh_weight = household.freq  # this is currently the av of person freq in the hh\n",
    "    freq = freq_sample(av_hh_weight, 10)\n",
    "\n",
    "    for idx in range(freq):\n",
    "        hh = deepcopy(household)\n",
    "        hh.hid = f\"{hh.hid}_{idx}\"\n",
    "        hh.people = {}\n",
    "        for pid, person in household.people.items():\n",
    "            p = deepcopy(person)\n",
    "            p.pid = f\"{pid}_{idx}\"\n",
    "            hh.add(p)\n",
    "        population_sample.add(hh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.061061Z",
     "start_time": "2020-11-11T11:50:18.895Z"
    }
   },
   "outputs": [],
   "source": [
    "population_sample.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facility Sampling¶ \n",
    "\n",
    "The facilities input is prepared using a separate project called OSM-Facility Sampler (OSMFS). This project woulbe be better names the OSM Facility *Extractor*. We use it to extract viable activity locations for each activity type for each zone. This project is not currently open source, but is described below:\n",
    "\n",
    "OSMFS joins osm data with the geographies of an area to create a mapping between zones, acts and facility locations (points). This is output as a geojson:\n",
    "\n",
    "{\"type\": \"FeatureCollection\", \"features\": [{\"id\": \"0\", \"type\": \"Feature\", \"properties\": {\"activity\": \"other\"}, \"geometry\": {\"type\": \"Point\", \"coordinates\": [-4.5235751, 54.1698685]}},\n",
    "\n",
    "todo: the current methodology does not support shared facilities, ie facilities with more than one activity (schools are places of education and work for example).\n",
    "\n",
    "todo: the above json has to be rejoined with the geography to create a spatial sampler. This is a duplicated operation which could be included in the Bench output, eg:\n",
    "\n",
    "zone_id: activity: (id, point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.074376Z",
     "start_time": "2020-11-11T11:50:18.971Z"
    }
   },
   "outputs": [],
   "source": [
    "from pam.samplers import facility\n",
    "import pam.write as write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.075895Z",
     "start_time": "2020-11-11T11:50:18.976Z"
    }
   },
   "outputs": [],
   "source": [
    "facilities_path = '/Users/fred.shone/Data/facilities/NTS_london_facilities.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.077513Z",
     "start_time": "2020-11-11T11:50:18.982Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_facilities(path, from_crs=\"EPSG:4326\", to_crs=\"EPSG:27700\"):\n",
    "    \n",
    "    facilities = gp.read_file(facilities_path)\n",
    "    facilities.crs = from_crs\n",
    "    facilities.to_crs(to_crs, inplace=True)\n",
    "    return facilities\n",
    "\n",
    "def load_zones(zones_path, from_crs=\"EPSG:27700\", to_crs=\"EPSG:27700\"):\n",
    "    \n",
    "    zones = gp.read_file(zones_path)\n",
    "    zones.set_index('Designated', inplace=True)\n",
    "    if not from_crs == to_crs:\n",
    "        zones.crs = from_crs\n",
    "        zones.to_crs(to_crs, inplace=True)\n",
    "    return zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.080414Z",
     "start_time": "2020-11-11T11:50:18.985Z"
    }
   },
   "outputs": [],
   "source": [
    "facilities = load_facilities(facilities_path)\n",
    "zones = load_zones(areas_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.093612Z",
     "start_time": "2020-11-11T11:50:18.989Z"
    }
   },
   "outputs": [],
   "source": [
    "facility_sampler = facility.FacilitySampler(\n",
    "    facilities=facilities,\n",
    "    zones=zones,\n",
    "    build_xml=True,\n",
    "    fail=False,\n",
    "    random_default=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.096794Z",
     "start_time": "2020-11-11T11:50:18.992Z"
    }
   },
   "outputs": [],
   "source": [
    "set(facility_sampler.activities) == population.activity_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.103220Z",
     "start_time": "2020-11-11T11:50:18.994Z"
    }
   },
   "outputs": [],
   "source": [
    "facility_sampler.clear()\n",
    "population_sample.sample_locs(facility_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.106902Z",
     "start_time": "2020-11-11T11:50:18.997Z"
    }
   },
   "outputs": [],
   "source": [
    "person = population_sample.random_person()\n",
    "person.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:48:33.844210Z",
     "start_time": "2020-11-11T11:48:33.815280Z"
    }
   },
   "source": [
    "## Write to Disk\n",
    "\n",
    "1. write MATSim formats to disk (plans and attributes)\n",
    "2. write csv and geojson summaries to disk\n",
    "3. write MATSim formatted facilities to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:50:42.112282Z",
     "start_time": "2020-11-11T11:50:18.999Z"
    }
   },
   "outputs": [],
   "source": [
    "comment = 'NTS london prelim 10nov2020 epsg27700'\n",
    "\n",
    "write.write_matsim(\n",
    "        population_sample,\n",
    "        plans_path=os.path.join(out_dir, 'plans.xml'),\n",
    "        attributes_path=os.path.join(out_dir, 'attributes.xml'),\n",
    "        comment=comment\n",
    "    )\n",
    "population_sample.to_csv(out_dir, crs=\"EPSG:27700\", to_crs=\"EPSG:4326\")\n",
    "facility_sampler.write_facilities_xml(os.path.join(out_dir, 'facilities.xml'), comment=comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
